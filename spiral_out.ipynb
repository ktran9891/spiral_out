{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktran/spiral_out/dependencies/dragonfly/utils/oper_utils.py:30: UserWarning: cannot import name direct\n",
      "Could not import Fortran direct library. Dragonfly can still be used, but might be slightly slower. To get rid of this warning, install a numpy compatible Fortran compiler (e.g. gfortran) and the python-dev package and reinstall Dragonfly.\n",
      "  warn('%s\\n%s'%(e, fortran_err_msg))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from dragonfly import maximise_function\n",
    "\n",
    "# Choose the GPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from style_transfer import transfer_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10582024966348522171\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Verify that we're using a GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parameters\n",
    "seed_image = './experimetal_results/gaius_baltar.png'\n",
    "attribute = 'humility'\n",
    "max_capital = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and results folders\n",
    "experiment_folder = ('./experimental_results/%s_%s/'\n",
    "                     % (attribute, seed_image.split('/')[-1].split('.')[0]))\n",
    "try:\n",
    "    os.mkdir(experiment_folder)\n",
    "except OSError:\n",
    "    pass\n",
    "art_folder = './art_images/'\n",
    "\n",
    "# Load the classifier\n",
    "classifier = load_model('./WikiArt-Emotions/resnet50.h5')\n",
    "\n",
    "# Figure out all the responses we're dealing with so we can make an attribute labeling map\n",
    "df = pd.read_pickle('./WikiArt-Emotions/data.pkl')\n",
    "responses = list(df.columns)\n",
    "responses.remove('Image URL')\n",
    "responses.sort()\n",
    "attr_map = {response: i for i, response in enumerate(responses)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_attribute_match(encoded_feature_vector, attribute):\n",
    "    # Perform style transfer\n",
    "    update_style(encoded_feature_vector)\n",
    "\n",
    "    # Use ResNet to classify the current image after style transfer\n",
    "    global current_image\n",
    "    probs = predict_adj_matches(current_image)\n",
    "\n",
    "    # Return the match between the current image and the target attribute\n",
    "    match = obj_fun(probs, attribute)\n",
    "    return match\n",
    "\n",
    "\n",
    "def update_style(encoded_feature_vector):\n",
    "    # Increment the counter for specifying which folder to save the images in\n",
    "    global updates\n",
    "    results_folder = experiment_folder + 'style_transfer_%03d' % updates\n",
    "    updates += 1\n",
    "    # Make the directory to store the style transfer progression (EAFP)\n",
    "    try:\n",
    "        os.mkdir(results_folder)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Fetch various information that we'll be using\n",
    "    global current_image\n",
    "\n",
    "    # Find the closest image to the vector provided, then transfer its style\n",
    "    global tree\n",
    "    global wga_names\n",
    "    _, image_index = tree.query(encoded_feature_vector)\n",
    "    style_image = wga_names[image_index]\n",
    "    style_path = art_folder + 'images/' + style_image\n",
    "    current_image = transfer_style(current_image, style_path,\n",
    "                                   result_folder=results_folder,\n",
    "                                   show=False)\n",
    "\n",
    "    # Remove the style image from the list of candidates (to avoid redos)\n",
    "    del wga_names[image_index]\n",
    "    global encoded_features\n",
    "    encoded_features = np.delete(encoded_features, image_index, axis=0)\n",
    "\n",
    "    # Update some global variables\n",
    "    global style_images\n",
    "    style_images.append(style_image)\n",
    "    tree = spatial.KDTree(encoded_features)\n",
    "\n",
    "\n",
    "def predict_adj_matches(img_path):\n",
    "    # Pull the image\n",
    "    img = image.load_img(img_path, target_size=(128, 128))\n",
    "\n",
    "    # Standardize/preprocess the image\n",
    "    array = image.img_to_array(img)\n",
    "    array -= np.mean(array, axis=2, keepdims=True)\n",
    "    array /= (np.std(array, axis=2, keepdims=True) + 1e-7)\n",
    "    array_expanded = np.expand_dims(array, axis=0)\n",
    "\n",
    "    # Feed the image to our trained ResNet model\n",
    "    probabilities = classifier.predict(array_expanded)[0, :]\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def obj_fun(probs, attribute):\n",
    "    attr_index = attr_map[attribute]\n",
    "    match = probs[attr_index]\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image names and initialize the list of style images\n",
    "with open(art_folder + 'image_mapping.txt', 'r') as file_handle:\n",
    "    wga_names = file_handle.readlines()\n",
    "wga_names = [name.split('\\n')[0] for name in wga_names]\n",
    "\n",
    "# Read the encoded features, then make a KDTree from them for matching\n",
    "h5f = h5py.File(art_folder + 'features.h5', 'r')\n",
    "encoded_features = h5f['encoded_features'][:]\n",
    "h5f.close()\n",
    "sys.setrecursionlimit(10000)  # So that we can actually build the tree\n",
    "tree = spatial.KDTree(encoded_features)\n",
    "\n",
    "# Figure out the domain for dragonfly to use\n",
    "maxes = encoded_features.max(axis=0)\n",
    "mins = encoded_features.min(axis=0)\n",
    "domain = [(min_, max_) for min_, max_ in zip(mins, maxes)]\n",
    "\n",
    "\n",
    "# Let's go!\n",
    "current_image = copy.deepcopy(seed_image)\n",
    "updates = 0\n",
    "style_images = []\n",
    "max_val, max_pt, history = maximise_function(lambda features: calc_attribute_match(features, attribute),\n",
    "                                             domain, max_capital)\n",
    "\n",
    "# Save it before we lose it!\n",
    "seed_name = seed_image.split('.')[0]\n",
    "with open(experiment_folder + 'history.pkl', 'wb') as file_handle:\n",
    "    pickle.dump((max_val, history, style_images), file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
