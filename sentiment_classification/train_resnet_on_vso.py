''' Adapted from `https://github.com/raghakot/keras-resnet/blob/master/cifar10.py` '''

import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping
import resnet


# Verify we're using a GPU
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())


# Find the data and figure out how many adjectives we have
data_dir = '/home/ktran/spiral_out/sentiment_classification/'
adjs = {adj for adj in os.listdir(data_dir + 'train/') + os.listdir(data_dir + 'test/')}
adjs.remove('.gitignore')

# Training hyperparameters
batch_size = 32
nb_classes = len(adjs)
nb_epoch = 200
n_training_samples = 336075  # Used to calculate number of steps per epoch
n_layers = 50
optimizer = 'adam'
loss = 'categorical_crossentropy'

# input image dimensions
target_size = (128, 128)
img_rows, img_cols = target_size
# The VSO images are RGB.
img_channels = 3 

# Training callbacks
lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)
early_stopper = EarlyStopping(min_delta=0.001, patience=10)
csv_logger = CSVLogger('resnet%i.csv' % n_layers)

# Build/compile ResNet
resnet_builder = getattr(resnet.ResnetBuilder, 'build_resnet_%i' % n_layers)
model = resnet_builder((img_channels, img_rows, img_cols), nb_classes)
model.compile(loss=loss,
              optimizer=optimizer,
              metrics=['accuracy'])

# This will do preprocessing and realtime data augmentation:
train_datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=True,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=True,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images
test_datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=True,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=True,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=False,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# We can't fit everything in memory, so flow the data from directories
train_generator = train_datagen.flow_from_directory(
        'train/',
        target_size=target_size,
        batch_size=batch_size,
        class_mode='categorical')
validation_generator = test_datagen.flow_from_directory(
        'test/',
        target_size=target_size,
        batch_size=batch_size,
        class_mode='categorical')

# Fit the model on the batches generated by the data generator
model.fit_generator(train_generator,
                    steps_per_epoch=n_training_samples // batch_size,
                    validation_data=validation_generator,
                    epochs=nb_epoch, verbose=1, max_queue_size=100,
                    callbacks=[lr_reducer, early_stopper, csv_logger])

# Save the model
model.compile(optimizer=optimizer, loss=loss)
model.save('resnet%i_vso.h5' % n_layers)
