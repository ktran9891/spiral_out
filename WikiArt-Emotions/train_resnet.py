import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping
from keras_preprocessing.image import ImageDataGenerator
import resnet


# Read the dataframe
df = pd.read_pickle('data.pkl')

# Figure out all the responses we're dealing with
responses = list(df.columns)
responses.remove('Image URL')
responses.sort()

# Add the file names to the dataframe
df['file names'] = [index + '.jpg' for index in df.index.values]

# This will do preprocessing and realtime data augmentation:
val_split = 0.2
target_size = (128, 128)
datagen = ImageDataGenerator(validation_split=val_split,
                             featurewise_center=False,
                             samplewise_center=True,
                             featurewise_std_normalization=False,
                             samplewise_std_normalization=True,
                             zca_whitening=False,
                             rotation_range=0,
                             width_shift_range=0.1,
                             height_shift_range=0.1,
                             horizontal_flip=True,
                             vertical_flip=False)


# Get the input
imgs = []
for file_name in tqdm(df['file names'], total=df.shape[0]):
    file_path = './images/' + file_name
    img = cv2.imread(file_path)
    img = cv2.resize(img, target_size)
    imgs.append(img)
imgs = np.array(imgs)

# Get the output
response_vectors = []
for index, row in df.iterrows():
    response_vector = np.array([row[response] for response in responses])
    response_vectors.append(response_vector)

# Put the data into the flow generator
train_generator = datagen.flow(x=imgs,
                               y=response_vectors,
                               batch_size=32,
                               subset='training')
validation_generator = datagen.flow(x=imgs,
                                    y=response_vectors,
                                    batch_size=32,
                                    subset='validation')

# Training hyperparameters
nb_classes = len(responses)
nb_epoch = 200
n_training_samples = df.shape[0]*(1-val_split)  # Used to calculate number of steps per epoch
n_layers = 50
optimizer = 'adam'
loss = 'categorical_crossentropy'

img_rows, img_cols = target_size
img_channels = 3 

# Training callbacks
lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)
early_stopper = EarlyStopping(min_delta=0.001, patience=10)

# Build/compile ResNet
resnet_builder = getattr(resnet.ResnetBuilder, 'build_resnet_%i' % n_layers)
model = resnet_builder((img_channels, img_rows, img_cols), nb_classes)
model.compile(loss=loss,
              optimizer=optimizer,
              metrics=['accuracy'])

# Fit the model on the batches generated by the data generator
model.fit_generator(train_generator,
                    steps_per_epoch=n_training_samples // 32,
                    validation_data=validation_generator,
                    epochs=nb_epoch, verbose=1, max_queue_size=100,
                    callbacks=[lr_reducer, early_stopper])

# Save the model
model.compile(optimizer=optimizer, loss=loss)
model.save('resnet%i.h5' % n_layers)
