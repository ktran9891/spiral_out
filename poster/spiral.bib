@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/He - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Kandasamy2019,
abstract = {Bayesian Optimisation (BO), refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently find the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.},
archivePrefix = {arXiv},
arxivId = {1903.06694},
author = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabas and Xing, Eric P.},
eprint = {1903.06694},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/Kandasamy - 2019 - Tuning Hyperparameters without Grad Students{\_}Scalable and Robust Bayesian Optimisation with Dragonfly.pdf:pdf},
title = {{Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly}},
url = {http://arxiv.org/abs/1903.06694},
year = {2019}
}
@article{Alvarez-Melis2017,
abstract = {We propose a neural network method for turning emotion into art. Our approach relies on a class-conditioned generative adversarial network trained on a dataset of modern artworks labeled with emotions. We generate this dataset through a large-scale user study of art perception with human subjects. Preliminary results show our framework generates images which, apart from aesthetically appealing, exhibit various features associated with the emotions they are conditioned on.},
author = {{Alvarez-Melis CSAIL}, David and Amores, Judith},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/Alvares-Melis - 2017 - The Emotional GAN{\_}Priming Adversarial Generation of Art with Emotion.pdf:pdf},
journal = {Nips},
number = {Nips},
title = {{The Emotional GAN: Priming Adversarial Generation of Art with Emotion}},
year = {2017}
}
@article{Mohammad2018,
abstract = {Art is imaginative human creation meant to be appreciated, make people think, and evoke an emotional response. Here for the first time, we create a dataset of more than 4,000 pieces of art (mostly paintings) that has annotations for emotions evoked in the observer. The pieces of art are selected from WikiArt.org's collection for four western styles (Renaissance Art, Post-Renaissance Art, Modern Art, and Contemporary Art). The art is annotated via crowdsourcing for one or more of twenty emotion categories (including neutral). In addition to emotions, the art is also annotated for whether it includes the depiction of a face and how much the observers like the art. The dataset, which we refer to as the WikiArt Emotions Dataset, can help answer several compelling questions, such as: what makes art evocative, how does art convey different emotions, what attributes of a painting make it well liked, what combinations of categories and emotions evoke strong emotional response, how much does the title of an art impact its emotional response, and what is the extent to which different categories of art evoke consistent emotions in people. We found that fear, happiness, love, and sadness were the dominant emotions that also obtained consistent annotations among the different annotators. We found that the title often impacts the affectual response to art. We show that pieces of art that depict faces draw more consistent emotional responses than those that do not. We also show, for each art category and emotion combination, the average agreements on the emotions evoked and the average art ratings. The WikiArt Emotions dataset also has applications in automatic image processing, as it can be used to develop systems that detect emotions evoked by art, and systems that can transform existing art (or even generate new art) that evokes the desired affectual response.},
author = {Mohammad, Saif M. and Kiritchenko, Svetlana},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/Mohammad - 2018 - WikiArt Emotions{\_}An Annotated Dataset of Emotions Evoked by Art.pdf:pdf},
journal = {Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)},
keywords = {Renaissance art,art,crowdsourcing,emotion analysis,emotions,image generation,image retrieval,images,modern art},
pages = {1225--1238},
title = {{WikiArt Emotions: An Annotated Dataset of Emotions Evoked by Art}},
url = {http://saifmohammad.com/WebDocs/lrec2018-paper-art-emotion.pdf{\%}0Asaifmohammad.com/WebDocs/lrec2018-paper-art-emotion.pdf},
year = {2018}
}
@misc{wga,
title = {{Web Gallery of Art}},
url = {https://www.wga.hu/},
year = {1996}
}
@article{Jou2016,
abstract = {Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of in-network regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with {\textgreater}40{\%} less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4{\%} over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.},
archivePrefix = {arXiv},
arxivId = {1604.01335},
author = {Jou, Brendan and Chang, Shih-Fu},
eprint = {1604.01335},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/Jou - 2016 - Deep Cross Residual Learning for Multitask Visual Recognition.pdf:pdf},
isbn = {9781450336031},
keywords = {cept detection,con-,deep networks,feature map illustration of,figure 1,generalization,multitask learning,regularization,residual,residual learning},
title = {{Deep Cross Residual Learning for Multitask Visual Recognition}},
url = {http://arxiv.org/abs/1604.01335},
year = {2016}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/KTran/Library/Application Support/Mendeley Desktop/Downloaded/Gatys - 2015 - Style Transfer.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
url = {http://arxiv.org/abs/1508.06576},
year = {2015}
}
